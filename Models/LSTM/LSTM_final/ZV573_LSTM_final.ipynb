{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import csv\n",
    "import keras\n",
    "#path constants\n",
    "train_path = '../../../iterations/data/final/train_norm'\n",
    "test_path = '../../../iterations/data/final/test_norm'\n",
    "\n",
    "cleaned_train_path = '../../../cleaned_data/train'\n",
    "cleaned_test_path = '../../../cleaned_data/test'\n",
    "#type constants\n",
    "vehicle_types = ['ZVe44', 'ZV573', 'ZV63d', 'ZVfd4', 'ZVa9c', 'ZVa78', 'ZV252']\n",
    "\n",
    "#two label dataframes\n",
    "train_label_df = pd.read_csv('../../../iterations/data/final' + '/label.csv', delimiter = ',', encoding = 'utf-8')\n",
    "test_label_df = pd.read_csv('../../../iterations/data/final' + '/label.csv', delimiter = ',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(filename, label_df):\n",
    "    idx = label_df.loc[label_df['sample_file_name'] == filename]\n",
    "    return idx.iloc[0]['label']\n",
    "\n",
    "def cal_length(path, vehicle_type, label_df):\n",
    "\n",
    "#vehicle_type: one string element under vehicle_types = ['ZVe44', 'ZV573', 'ZV63d', 'ZVfd4', 'ZVa9c', 'ZVa78', 'ZV252']\n",
    "    path = path + '/' + vehicle_type\n",
    "    #these are variables to calculate traversing progress (DO NOT CHANGE)\n",
    "    counts_per_percent = int(len(os.listdir(path)) / 100)\n",
    "    percentage_completion = 0\n",
    "    counter = 0\n",
    "    \n",
    "    single_len=0\n",
    "    file_count=0\n",
    "    file_len=0\n",
    "    for file in os.listdir(path):\n",
    "        sample_df = pd.read_csv(path + '/' + file, delimiter = ',', encoding = 'utf-8')\n",
    "        file_len+=len(sample_df)\n",
    "        file_count+=1\n",
    "        if len(sample_df)>=single_len:\n",
    "            single_len=len(sample_df)\n",
    "    ave_file_len=file_len/file_count\n",
    "    print(\"AVG Length: \",ave_file_len,' file count: ',file_count,' max_len: ',single_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG Length:  154.8896184416482  file count:  49193  max_len:  5355\n"
     ]
    }
   ],
   "source": [
    "cal_length(train_path,'ZV573',train_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_len_weighted=round(392*0.7)\n",
    "file_len_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(filename, label_df):\n",
    "    idx = label_df.loc[label_df['sample_file_name'] == filename]\n",
    "    return idx.iloc[0]['label']\n",
    "\n",
    "def TraverseFiles(path, vehicle_type, label_df, length):\n",
    "\n",
    "#vehicle_type: one string element under vehicle_types = ['ZVe44', 'ZV573', 'ZV63d', 'ZVfd4', 'ZVa9c', 'ZVa78', 'ZV252']\n",
    "    path = path + '/' + vehicle_type\n",
    "    #these are variables to calculate traversing progress (DO NOT CHANGE)\n",
    "    counts_per_percent = int(len(os.listdir(path)) / 100)\n",
    "    percentage_completion = 0\n",
    "    counter = 0\n",
    "    \n",
    "    lables=np.array([])\n",
    "    file_list= []\n",
    "    #file_len=file_len_weighted\n",
    "    for file in os.listdir(path):\n",
    "        sample_df = pd.read_csv(path + '/' + file, delimiter = ',', encoding = 'utf-8')\n",
    "        if len(sample_df)==0:\n",
    "            continue\n",
    "        elif len(sample_df)!=0:\n",
    "            sample_array=np.array(sample_df.iloc[:,0:11])\n",
    "            if len(sample_array)<length:\n",
    "                n_zeros=length-len(sample_array)\n",
    "                sample_array=np.concatenate((np.zeros((n_zeros,11)),sample_array))\n",
    "                #print(len(sample_array))\n",
    "            elif len(sample_array)>length:\n",
    "                sample_array=sample_array[0:length]\n",
    "                #print(len(sample_array))\n",
    "            elif len(sample_array)==length:\n",
    "                sample_array=sample_array\n",
    "                #print(len(sample_array))\n",
    "            #np.float(sample_array)\n",
    "            #sample_array=torch.from_numpy(sample_array).float()\n",
    "            #file_list=torch.cat((file_list,sample_array[0:76,]), dim=1)\n",
    "        file_list.append(sample_array)\n",
    "        l=np.array(label_df.loc[label_df['sample_file_name'] == file])\n",
    "        lables=np.append(lables,[l[:,1]])\n",
    "        #belows are to show traversing progress (DO NOT CHANGE)\n",
    "        counter += 1\n",
    "        if counter == counts_per_percent:\n",
    "            counter = 0\n",
    "            percentage_completion += 1\n",
    "            print('traversing files under', path, ':', percentage_completion, \"%\", end=\"\\r\", flush=True)\n",
    "\n",
    "    return file_list,lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traversing files under ../../../iterations/data/final/test_norm/ZV573 : 100 %%\r"
     ]
    }
   ],
   "source": [
    "data_array_train, data_labels_train = TraverseFiles(train_path,'ZV573',train_label_df,155)\n",
    "data_array_test, data_labels_test = TraverseFiles(test_path,'ZV573',test_label_df,155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(data_array_train)\n",
    "labels_train=np.array(data_labels_train,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array(data_array_test)\n",
    "labels_test=np.array(data_labels_test,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49193, 155, 11)\n",
      "(49193,)\n",
      "(12299, 155, 11)\n",
      "(12299,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(x_test.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "lstm = keras.Sequential()\n",
    "lstm.add(keras.layers.Dropout(0.2))\n",
    "lstm.add(keras.layers.LSTM(16,dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
    "lstm.add(keras.layers.BatchNormalization())\n",
    "lstm.add(keras.layers.LSTM(16,dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm.add(keras.layers.BatchNormalization())\n",
    "lstm.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "49193/49193 [==============================] - 39s 795us/step - loss: 0.7247 - accuracy: 0.5126\n",
      "Epoch 2/500\n",
      "49193/49193 [==============================] - 40s 805us/step - loss: 0.6975 - accuracy: 0.5173\n",
      "Epoch 3/500\n",
      "49193/49193 [==============================] - 40s 817us/step - loss: 0.6926 - accuracy: 0.5251\n",
      "Epoch 4/500\n",
      "49193/49193 [==============================] - 40s 806us/step - loss: 0.6916 - accuracy: 0.5288\n",
      "Epoch 5/500\n",
      "49193/49193 [==============================] - 41s 824us/step - loss: 0.6880 - accuracy: 0.5374\n",
      "Epoch 6/500\n",
      "49193/49193 [==============================] - 41s 834us/step - loss: 0.6870 - accuracy: 0.5405\n",
      "Epoch 7/500\n",
      "49193/49193 [==============================] - 41s 840us/step - loss: 0.6842 - accuracy: 0.5502\n",
      "Epoch 8/500\n",
      "49193/49193 [==============================] - 39s 789us/step - loss: 0.6831 - accuracy: 0.5482\n",
      "Epoch 9/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6822 - accuracy: 0.5528\n",
      "Epoch 10/500\n",
      "49193/49193 [==============================] - 39s 784us/step - loss: 0.6809 - accuracy: 0.5554\n",
      "Epoch 11/500\n",
      "49193/49193 [==============================] - 38s 770us/step - loss: 0.6809 - accuracy: 0.5545\n",
      "Epoch 12/500\n",
      "49193/49193 [==============================] - 37s 761us/step - loss: 0.6793 - accuracy: 0.5575\n",
      "Epoch 13/500\n",
      "49193/49193 [==============================] - 36s 722us/step - loss: 0.6795 - accuracy: 0.5562\n",
      "Epoch 14/500\n",
      "49193/49193 [==============================] - 36s 723us/step - loss: 0.6793 - accuracy: 0.5575\n",
      "Epoch 15/500\n",
      "49193/49193 [==============================] - 36s 722us/step - loss: 0.6779 - accuracy: 0.5618\n",
      "Epoch 16/500\n",
      "49193/49193 [==============================] - 36s 725us/step - loss: 0.6775 - accuracy: 0.5629\n",
      "Epoch 17/500\n",
      "49193/49193 [==============================] - 36s 726us/step - loss: 0.6772 - accuracy: 0.5641\n",
      "Epoch 18/500\n",
      "49193/49193 [==============================] - 36s 740us/step - loss: 0.6775 - accuracy: 0.5627\n",
      "Epoch 19/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6781 - accuracy: 0.5611\n",
      "Epoch 20/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6755 - accuracy: 0.5678\n",
      "Epoch 21/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6757 - accuracy: 0.5655\n",
      "Epoch 22/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6763 - accuracy: 0.5639\n",
      "Epoch 23/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6754 - accuracy: 0.5650\n",
      "Epoch 24/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6755 - accuracy: 0.5644\n",
      "Epoch 25/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6749 - accuracy: 0.5666\n",
      "Epoch 26/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6743 - accuracy: 0.5686\n",
      "Epoch 27/500\n",
      "49193/49193 [==============================] - 36s 740us/step - loss: 0.6756 - accuracy: 0.5647\n",
      "Epoch 28/500\n",
      "49193/49193 [==============================] - 37s 747us/step - loss: 0.6740 - accuracy: 0.5687\n",
      "Epoch 29/500\n",
      "49193/49193 [==============================] - 36s 742us/step - loss: 0.6738 - accuracy: 0.5670\n",
      "Epoch 30/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6736 - accuracy: 0.5694\n",
      "Epoch 31/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6734 - accuracy: 0.5708\n",
      "Epoch 32/500\n",
      "49193/49193 [==============================] - 37s 745us/step - loss: 0.6742 - accuracy: 0.5666\n",
      "Epoch 33/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6736 - accuracy: 0.5684\n",
      "Epoch 34/500\n",
      "49193/49193 [==============================] - 37s 742us/step - loss: 0.6728 - accuracy: 0.5697\n",
      "Epoch 35/500\n",
      "49193/49193 [==============================] - 36s 740us/step - loss: 0.6728 - accuracy: 0.5694\n",
      "Epoch 36/500\n",
      "49193/49193 [==============================] - 38s 776us/step - loss: 0.6730 - accuracy: 0.5703\n",
      "Epoch 37/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6730 - accuracy: 0.5711\n",
      "Epoch 38/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6727 - accuracy: 0.5698\n",
      "Epoch 39/500\n",
      "49193/49193 [==============================] - 37s 751us/step - loss: 0.6726 - accuracy: 0.5685\n",
      "Epoch 40/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6727 - accuracy: 0.5693\n",
      "Epoch 41/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6724 - accuracy: 0.5696\n",
      "Epoch 42/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6722 - accuracy: 0.5714\n",
      "Epoch 43/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6705 - accuracy: 0.5732\n",
      "Epoch 44/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6714 - accuracy: 0.5697\n",
      "Epoch 45/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6713 - accuracy: 0.5715\n",
      "Epoch 46/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6719 - accuracy: 0.5717\n",
      "Epoch 47/500\n",
      "49193/49193 [==============================] - 36s 740us/step - loss: 0.6712 - accuracy: 0.5703\n",
      "Epoch 48/500\n",
      "49193/49193 [==============================] - 37s 743us/step - loss: 0.6708 - accuracy: 0.5717\n",
      "Epoch 49/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6703 - accuracy: 0.5739\n",
      "Epoch 50/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6712 - accuracy: 0.5726\n",
      "Epoch 51/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6710 - accuracy: 0.5729\n",
      "Epoch 52/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6713 - accuracy: 0.5704\n",
      "Epoch 53/500\n",
      "49193/49193 [==============================] - 36s 740us/step - loss: 0.6713 - accuracy: 0.5725\n",
      "Epoch 54/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6708 - accuracy: 0.5730\n",
      "Epoch 55/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6707 - accuracy: 0.5739\n",
      "Epoch 56/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6707 - accuracy: 0.5726\n",
      "Epoch 57/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6710 - accuracy: 0.5697\n",
      "Epoch 58/500\n",
      "49193/49193 [==============================] - 37s 745us/step - loss: 0.6703 - accuracy: 0.5722\n",
      "Epoch 59/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6694 - accuracy: 0.5735\n",
      "Epoch 60/500\n",
      "49193/49193 [==============================] - 37s 759us/step - loss: 0.6710 - accuracy: 0.5705\n",
      "Epoch 61/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6698 - accuracy: 0.5723\n",
      "Epoch 62/500\n",
      "49193/49193 [==============================] - 37s 761us/step - loss: 0.6703 - accuracy: 0.5735\n",
      "Epoch 63/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6705 - accuracy: 0.5736\n",
      "Epoch 64/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6700 - accuracy: 0.5736\n",
      "Epoch 65/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6705 - accuracy: 0.5739\n",
      "Epoch 66/500\n",
      "49193/49193 [==============================] - 37s 750us/step - loss: 0.6707 - accuracy: 0.5711\n",
      "Epoch 67/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6707 - accuracy: 0.5715\n",
      "Epoch 68/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6690 - accuracy: 0.5743\n",
      "Epoch 69/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6703 - accuracy: 0.5732\n",
      "Epoch 70/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6691 - accuracy: 0.5728\n",
      "Epoch 71/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6685 - accuracy: 0.5741\n",
      "Epoch 72/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6684 - accuracy: 0.5737\n",
      "Epoch 73/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6673 - accuracy: 0.5740\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6662 - accuracy: 0.5750\n",
      "Epoch 75/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6663 - accuracy: 0.5742\n",
      "Epoch 76/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6650 - accuracy: 0.5794\n",
      "Epoch 77/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6655 - accuracy: 0.5775\n",
      "Epoch 78/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6650 - accuracy: 0.5790\n",
      "Epoch 79/500\n",
      "49193/49193 [==============================] - 36s 738us/step - loss: 0.6651 - accuracy: 0.5768\n",
      "Epoch 80/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6627 - accuracy: 0.5807\n",
      "Epoch 81/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6640 - accuracy: 0.5788\n",
      "Epoch 82/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6639 - accuracy: 0.5819\n",
      "Epoch 83/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6637 - accuracy: 0.5820\n",
      "Epoch 84/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6633 - accuracy: 0.5814\n",
      "Epoch 85/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6635 - accuracy: 0.5799\n",
      "Epoch 86/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6633 - accuracy: 0.5816\n",
      "Epoch 87/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6632 - accuracy: 0.5792\n",
      "Epoch 88/500\n",
      "49193/49193 [==============================] - 37s 747us/step - loss: 0.6629 - accuracy: 0.5813\n",
      "Epoch 89/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6633 - accuracy: 0.5831\n",
      "Epoch 90/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6629 - accuracy: 0.5796\n",
      "Epoch 91/500\n",
      "49193/49193 [==============================] - 37s 742us/step - loss: 0.6631 - accuracy: 0.5827\n",
      "Epoch 92/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6638 - accuracy: 0.5782\n",
      "Epoch 93/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6626 - accuracy: 0.5818\n",
      "Epoch 94/500\n",
      "49193/49193 [==============================] - 37s 759us/step - loss: 0.6634 - accuracy: 0.5817\n",
      "Epoch 95/500\n",
      "49193/49193 [==============================] - 37s 745us/step - loss: 0.6626 - accuracy: 0.5809\n",
      "Epoch 96/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6610 - accuracy: 0.5851\n",
      "Epoch 97/500\n",
      "49193/49193 [==============================] - 37s 750us/step - loss: 0.6627 - accuracy: 0.5825\n",
      "Epoch 98/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6623 - accuracy: 0.5834s - loss:\n",
      "Epoch 99/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6619 - accuracy: 0.5834\n",
      "Epoch 100/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6626 - accuracy: 0.5822\n",
      "Epoch 101/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6626 - accuracy: 0.5819\n",
      "Epoch 102/500\n",
      "49193/49193 [==============================] - 37s 760us/step - loss: 0.6619 - accuracy: 0.5818\n",
      "Epoch 103/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6622 - accuracy: 0.5805\n",
      "Epoch 104/500\n",
      "49193/49193 [==============================] - 37s 759us/step - loss: 0.6618 - accuracy: 0.5852\n",
      "Epoch 105/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6603 - accuracy: 0.5860\n",
      "Epoch 106/500\n",
      "49193/49193 [==============================] - 37s 750us/step - loss: 0.6619 - accuracy: 0.5823\n",
      "Epoch 107/500\n",
      "49193/49193 [==============================] - 37s 748us/step - loss: 0.6610 - accuracy: 0.5840\n",
      "Epoch 108/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6620 - accuracy: 0.5813\n",
      "Epoch 109/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6593 - accuracy: 0.5863\n",
      "Epoch 110/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6607 - accuracy: 0.5862\n",
      "Epoch 111/500\n",
      "49193/49193 [==============================] - 37s 757us/step - loss: 0.6615 - accuracy: 0.5888\n",
      "Epoch 112/500\n",
      "49193/49193 [==============================] - 37s 757us/step - loss: 0.6616 - accuracy: 0.5840\n",
      "Epoch 113/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6619 - accuracy: 0.5855\n",
      "Epoch 114/500\n",
      "49193/49193 [==============================] - 37s 751us/step - loss: 0.6617 - accuracy: 0.5817\n",
      "Epoch 115/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6617 - accuracy: 0.5798\n",
      "Epoch 116/500\n",
      "49193/49193 [==============================] - 37s 759us/step - loss: 0.6602 - accuracy: 0.5897\n",
      "Epoch 117/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6602 - accuracy: 0.5878\n",
      "Epoch 118/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6605 - accuracy: 0.5839\n",
      "Epoch 119/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6609 - accuracy: 0.5881\n",
      "Epoch 120/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6604 - accuracy: 0.5846\n",
      "Epoch 121/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6607 - accuracy: 0.5864\n",
      "Epoch 122/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6604 - accuracy: 0.5871\n",
      "Epoch 123/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6594 - accuracy: 0.5867\n",
      "Epoch 124/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6600 - accuracy: 0.5876\n",
      "Epoch 125/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6605 - accuracy: 0.5844\n",
      "Epoch 126/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6593 - accuracy: 0.5885\n",
      "Epoch 127/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6602 - accuracy: 0.5851\n",
      "Epoch 128/500\n",
      "49193/49193 [==============================] - 37s 748us/step - loss: 0.6592 - accuracy: 0.5867\n",
      "Epoch 129/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6596 - accuracy: 0.5879\n",
      "Epoch 130/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6586 - accuracy: 0.5896\n",
      "Epoch 131/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6589 - accuracy: 0.5898\n",
      "Epoch 132/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6586 - accuracy: 0.5883\n",
      "Epoch 133/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6601 - accuracy: 0.5848\n",
      "Epoch 134/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6589 - accuracy: 0.5920\n",
      "Epoch 135/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6583 - accuracy: 0.5873\n",
      "Epoch 136/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6593 - accuracy: 0.5895\n",
      "Epoch 137/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6594 - accuracy: 0.5855\n",
      "Epoch 138/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6582 - accuracy: 0.5904\n",
      "Epoch 139/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6599 - accuracy: 0.5889\n",
      "Epoch 140/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6590 - accuracy: 0.5884\n",
      "Epoch 141/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6591 - accuracy: 0.5880\n",
      "Epoch 142/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6599 - accuracy: 0.5869\n",
      "Epoch 143/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6582 - accuracy: 0.5881\n",
      "Epoch 144/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6594 - accuracy: 0.5872\n",
      "Epoch 145/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6592 - accuracy: 0.5875\n",
      "Epoch 146/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6585 - accuracy: 0.5870\n",
      "Epoch 147/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6586 - accuracy: 0.5883\n",
      "Epoch 148/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6591 - accuracy: 0.5849\n",
      "Epoch 149/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6576 - accuracy: 0.5938\n",
      "Epoch 150/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6587 - accuracy: 0.5860\n",
      "Epoch 151/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6568 - accuracy: 0.5929\n",
      "Epoch 152/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6583 - accuracy: 0.5883\n",
      "Epoch 153/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6596 - accuracy: 0.5879\n",
      "Epoch 154/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6588 - accuracy: 0.5905\n",
      "Epoch 155/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6579 - accuracy: 0.5909\n",
      "Epoch 156/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6575 - accuracy: 0.5908\n",
      "Epoch 157/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6578 - accuracy: 0.5909\n",
      "Epoch 158/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6598 - accuracy: 0.5883\n",
      "Epoch 159/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6583 - accuracy: 0.5901\n",
      "Epoch 160/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6574 - accuracy: 0.5900\n",
      "Epoch 161/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6581 - accuracy: 0.5890\n",
      "Epoch 162/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6576 - accuracy: 0.5915\n",
      "Epoch 163/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6577 - accuracy: 0.5889\n",
      "Epoch 164/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6574 - accuracy: 0.5923\n",
      "Epoch 165/500\n",
      "49193/49193 [==============================] - 36s 739us/step - loss: 0.6573 - accuracy: 0.5919\n",
      "Epoch 166/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6571 - accuracy: 0.5926\n",
      "Epoch 167/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6571 - accuracy: 0.5924\n",
      "Epoch 168/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6565 - accuracy: 0.5921\n",
      "Epoch 169/500\n",
      "49193/49193 [==============================] - 37s 750us/step - loss: 0.6572 - accuracy: 0.5895\n",
      "Epoch 170/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6564 - accuracy: 0.5925\n",
      "Epoch 171/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6575 - accuracy: 0.5913\n",
      "Epoch 172/500\n",
      "49193/49193 [==============================] - 37s 747us/step - loss: 0.6586 - accuracy: 0.5879\n",
      "Epoch 173/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6573 - accuracy: 0.5897\n",
      "Epoch 174/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6566 - accuracy: 0.5948\n",
      "Epoch 175/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6568 - accuracy: 0.5914\n",
      "Epoch 176/500\n",
      "49193/49193 [==============================] - 37s 751us/step - loss: 0.6580 - accuracy: 0.5900\n",
      "Epoch 177/500\n",
      "49193/49193 [==============================] - 37s 750us/step - loss: 0.6578 - accuracy: 0.5904\n",
      "Epoch 178/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6563 - accuracy: 0.5922\n",
      "Epoch 179/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6583 - accuracy: 0.5886\n",
      "Epoch 180/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6561 - accuracy: 0.5907\n",
      "Epoch 181/500\n",
      "49193/49193 [==============================] - 37s 742us/step - loss: 0.6564 - accuracy: 0.5927\n",
      "Epoch 182/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6568 - accuracy: 0.5939\n",
      "Epoch 183/500\n",
      "49193/49193 [==============================] - 37s 750us/step - loss: 0.6570 - accuracy: 0.5889\n",
      "Epoch 184/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6571 - accuracy: 0.5912\n",
      "Epoch 185/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6570 - accuracy: 0.5915\n",
      "Epoch 186/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6555 - accuracy: 0.5952\n",
      "Epoch 187/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6567 - accuracy: 0.5914\n",
      "Epoch 188/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6563 - accuracy: 0.5915\n",
      "Epoch 189/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6561 - accuracy: 0.5932\n",
      "Epoch 190/500\n",
      "49193/49193 [==============================] - 38s 766us/step - loss: 0.6557 - accuracy: 0.5936\n",
      "Epoch 191/500\n",
      "49193/49193 [==============================] - 38s 766us/step - loss: 0.6568 - accuracy: 0.5933\n",
      "Epoch 192/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6563 - accuracy: 0.5926\n",
      "Epoch 193/500\n",
      "49193/49193 [==============================] - 36s 742us/step - loss: 0.6567 - accuracy: 0.5916\n",
      "Epoch 194/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6574 - accuracy: 0.5916\n",
      "Epoch 195/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6558 - accuracy: 0.5946\n",
      "Epoch 196/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6568 - accuracy: 0.5915\n",
      "Epoch 197/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6562 - accuracy: 0.5955\n",
      "Epoch 198/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6554 - accuracy: 0.5943\n",
      "Epoch 199/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6571 - accuracy: 0.5925\n",
      "Epoch 200/500\n",
      "49193/49193 [==============================] - 37s 743us/step - loss: 0.6558 - accuracy: 0.5945\n",
      "Epoch 201/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6561 - accuracy: 0.5934\n",
      "Epoch 202/500\n",
      "49193/49193 [==============================] - 37s 759us/step - loss: 0.6548 - accuracy: 0.5956\n",
      "Epoch 203/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6555 - accuracy: 0.5939\n",
      "Epoch 204/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6560 - accuracy: 0.5936\n",
      "Epoch 205/500\n",
      "49193/49193 [==============================] - 37s 757us/step - loss: 0.6560 - accuracy: 0.5944\n",
      "Epoch 206/500\n",
      "49193/49193 [==============================] - 37s 759us/step - loss: 0.6565 - accuracy: 0.5912\n",
      "Epoch 207/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6559 - accuracy: 0.5940\n",
      "Epoch 208/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6558 - accuracy: 0.5932\n",
      "Epoch 209/500\n",
      "49193/49193 [==============================] - 37s 757us/step - loss: 0.6555 - accuracy: 0.5971\n",
      "Epoch 210/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6559 - accuracy: 0.5947\n",
      "Epoch 211/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6562 - accuracy: 0.5932\n",
      "Epoch 212/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6559 - accuracy: 0.5921\n",
      "Epoch 213/500\n",
      "49193/49193 [==============================] - 37s 752us/step - loss: 0.6558 - accuracy: 0.5939\n",
      "Epoch 214/500\n",
      "49193/49193 [==============================] - 36s 734us/step - loss: 0.6556 - accuracy: 0.5941\n",
      "Epoch 215/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6553 - accuracy: 0.5939\n",
      "Epoch 216/500\n",
      "49193/49193 [==============================] - 37s 746us/step - loss: 0.6568 - accuracy: 0.5940\n",
      "Epoch 217/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6549 - accuracy: 0.5942\n",
      "Epoch 218/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6552 - accuracy: 0.5952\n",
      "Epoch 219/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6549 - accuracy: 0.5944\n",
      "Epoch 220/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6557 - accuracy: 0.5938\n",
      "Epoch 221/500\n",
      "49193/49193 [==============================] - 37s 757us/step - loss: 0.6561 - accuracy: 0.5942\n",
      "Epoch 222/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6539 - accuracy: 0.5972\n",
      "Epoch 223/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6558 - accuracy: 0.5931\n",
      "Epoch 224/500\n",
      "49193/49193 [==============================] - 37s 757us/step - loss: 0.6548 - accuracy: 0.5964\n",
      "Epoch 225/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6545 - accuracy: 0.5970\n",
      "Epoch 226/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6546 - accuracy: 0.5962\n",
      "Epoch 227/500\n",
      "49193/49193 [==============================] - 37s 761us/step - loss: 0.6548 - accuracy: 0.5944\n",
      "Epoch 228/500\n",
      "49193/49193 [==============================] - 38s 765us/step - loss: 0.6546 - accuracy: 0.5939\n",
      "Epoch 229/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6542 - accuracy: 0.5951\n",
      "Epoch 230/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6545 - accuracy: 0.5940\n",
      "Epoch 231/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6549 - accuracy: 0.5952\n",
      "Epoch 232/500\n",
      "49193/49193 [==============================] - 37s 749us/step - loss: 0.6542 - accuracy: 0.5959\n",
      "Epoch 233/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6547 - accuracy: 0.5953\n",
      "Epoch 234/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6540 - accuracy: 0.5960\n",
      "Epoch 235/500\n",
      "49193/49193 [==============================] - 37s 756us/step - loss: 0.6546 - accuracy: 0.5949\n",
      "Epoch 236/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6544 - accuracy: 0.5951\n",
      "Epoch 237/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6554 - accuracy: 0.5964\n",
      "Epoch 238/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6546 - accuracy: 0.5953\n",
      "Epoch 239/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6553 - accuracy: 0.5956\n",
      "Epoch 240/500\n",
      "49193/49193 [==============================] - 37s 754us/step - loss: 0.6551 - accuracy: 0.5967\n",
      "Epoch 241/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6538 - accuracy: 0.5989\n",
      "Epoch 242/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6540 - accuracy: 0.5966\n",
      "Epoch 243/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6544 - accuracy: 0.5986\n",
      "Epoch 244/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6547 - accuracy: 0.5992\n",
      "Epoch 245/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6534 - accuracy: 0.5958\n",
      "Epoch 246/500\n",
      "49193/49193 [==============================] - 37s 758us/step - loss: 0.6547 - accuracy: 0.5944\n",
      "Epoch 247/500\n",
      "49193/49193 [==============================] - 37s 753us/step - loss: 0.6542 - accuracy: 0.5969\n",
      "Epoch 248/500\n",
      "49193/49193 [==============================] - 37s 760us/step - loss: 0.6544 - accuracy: 0.5962\n",
      "Epoch 249/500\n",
      "49193/49193 [==============================] - 37s 744us/step - loss: 0.6545 - accuracy: 0.5963\n",
      "Epoch 250/500\n",
      "49193/49193 [==============================] - 36s 740us/step - loss: 0.6540 - accuracy: 0.5969\n",
      "Epoch 251/500\n",
      "49193/49193 [==============================] - 37s 742us/step - loss: 0.6538 - accuracy: 0.5981\n",
      "Epoch 252/500\n",
      "49193/49193 [==============================] - 37s 751us/step - loss: 0.6545 - accuracy: 0.5948\n",
      "Epoch 253/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6544 - accuracy: 0.5969\n",
      "Epoch 254/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6537 - accuracy: 0.5964\n",
      "Epoch 255/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6542 - accuracy: 0.5959\n",
      "Epoch 256/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6549 - accuracy: 0.5958\n",
      "Epoch 257/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6546 - accuracy: 0.5978\n",
      "Epoch 258/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6547 - accuracy: 0.5955\n",
      "Epoch 259/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6548 - accuracy: 0.5971\n",
      "Epoch 260/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6534 - accuracy: 0.5972\n",
      "Epoch 261/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6544 - accuracy: 0.5953\n",
      "Epoch 262/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6558 - accuracy: 0.5935\n",
      "Epoch 263/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6536 - accuracy: 0.5974\n",
      "Epoch 264/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6546 - accuracy: 0.5947\n",
      "Epoch 265/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6547 - accuracy: 0.5952\n",
      "Epoch 266/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6536 - accuracy: 0.5992\n",
      "Epoch 267/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6529 - accuracy: 0.5983\n",
      "Epoch 268/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6542 - accuracy: 0.5961\n",
      "Epoch 269/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6553 - accuracy: 0.5956\n",
      "Epoch 270/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6545 - accuracy: 0.5982\n",
      "Epoch 271/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6532 - accuracy: 0.5966\n",
      "Epoch 272/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6534 - accuracy: 0.5964\n",
      "Epoch 273/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6540 - accuracy: 0.5961\n",
      "Epoch 274/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6542 - accuracy: 0.5990\n",
      "Epoch 275/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6529 - accuracy: 0.5976\n",
      "Epoch 276/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6535 - accuracy: 0.5960\n",
      "Epoch 277/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6544 - accuracy: 0.5973\n",
      "Epoch 278/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6528 - accuracy: 0.5988\n",
      "Epoch 279/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6527 - accuracy: 0.5990\n",
      "Epoch 280/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6534 - accuracy: 0.5998\n",
      "Epoch 281/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6533 - accuracy: 0.5967\n",
      "Epoch 282/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6530 - accuracy: 0.6005\n",
      "Epoch 283/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6547 - accuracy: 0.5946\n",
      "Epoch 284/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6536 - accuracy: 0.5974\n",
      "Epoch 285/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6541 - accuracy: 0.5967\n",
      "Epoch 286/500\n",
      "49193/49193 [==============================] - 36s 739us/step - loss: 0.6539 - accuracy: 0.5975\n",
      "Epoch 287/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6541 - accuracy: 0.5948\n",
      "Epoch 288/500\n",
      "49193/49193 [==============================] - 36s 726us/step - loss: 0.6538 - accuracy: 0.5962\n",
      "Epoch 289/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6543 - accuracy: 0.5966\n",
      "Epoch 290/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6529 - accuracy: 0.6014\n",
      "Epoch 291/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6541 - accuracy: 0.5978\n",
      "Epoch 292/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6536 - accuracy: 0.5985\n",
      "Epoch 293/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6530 - accuracy: 0.5993\n",
      "Epoch 294/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6542 - accuracy: 0.5961\n",
      "Epoch 295/500\n",
      "49193/49193 [==============================] - 37s 755us/step - loss: 0.6530 - accuracy: 0.5981\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6536 - accuracy: 0.5953\n",
      "Epoch 297/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6541 - accuracy: 0.5958\n",
      "Epoch 298/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6523 - accuracy: 0.6004\n",
      "Epoch 299/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6532 - accuracy: 0.5975\n",
      "Epoch 300/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6537 - accuracy: 0.5976\n",
      "Epoch 301/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6540 - accuracy: 0.5964\n",
      "Epoch 302/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6537 - accuracy: 0.5961\n",
      "Epoch 303/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6536 - accuracy: 0.5981\n",
      "Epoch 304/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6531 - accuracy: 0.5975\n",
      "Epoch 305/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6537 - accuracy: 0.5968\n",
      "Epoch 306/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6532 - accuracy: 0.5973\n",
      "Epoch 307/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6518 - accuracy: 0.6007\n",
      "Epoch 308/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6527 - accuracy: 0.6000\n",
      "Epoch 309/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6531 - accuracy: 0.5986\n",
      "Epoch 310/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6523 - accuracy: 0.5996\n",
      "Epoch 311/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6535 - accuracy: 0.5973\n",
      "Epoch 312/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6529 - accuracy: 0.5982\n",
      "Epoch 313/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6530 - accuracy: 0.5991\n",
      "Epoch 314/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6534 - accuracy: 0.5966\n",
      "Epoch 315/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6520 - accuracy: 0.5981\n",
      "Epoch 316/500\n",
      "49193/49193 [==============================] - 36s 726us/step - loss: 0.6524 - accuracy: 0.5960\n",
      "Epoch 317/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6530 - accuracy: 0.5987\n",
      "Epoch 318/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6532 - accuracy: 0.5975\n",
      "Epoch 319/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6534 - accuracy: 0.5978\n",
      "Epoch 320/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6533 - accuracy: 0.5973\n",
      "Epoch 321/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6526 - accuracy: 0.5991\n",
      "Epoch 322/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6530 - accuracy: 0.5986\n",
      "Epoch 323/500\n",
      "49193/49193 [==============================] - 36s 732us/step - loss: 0.6532 - accuracy: 0.5946\n",
      "Epoch 324/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6531 - accuracy: 0.5994\n",
      "Epoch 325/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6544 - accuracy: 0.5962\n",
      "Epoch 326/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6535 - accuracy: 0.5980\n",
      "Epoch 327/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6528 - accuracy: 0.5982\n",
      "Epoch 328/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6522 - accuracy: 0.5975\n",
      "Epoch 329/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6523 - accuracy: 0.5991\n",
      "Epoch 330/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6525 - accuracy: 0.5994\n",
      "Epoch 331/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6535 - accuracy: 0.5962\n",
      "Epoch 332/500\n",
      "49193/49193 [==============================] - 36s 738us/step - loss: 0.6529 - accuracy: 0.6007\n",
      "Epoch 333/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6533 - accuracy: 0.5979\n",
      "Epoch 334/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6531 - accuracy: 0.5975\n",
      "Epoch 335/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6515 - accuracy: 0.6010\n",
      "Epoch 336/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6531 - accuracy: 0.5980\n",
      "Epoch 337/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6526 - accuracy: 0.5984\n",
      "Epoch 338/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6526 - accuracy: 0.5976\n",
      "Epoch 339/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6532 - accuracy: 0.5996\n",
      "Epoch 340/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6521 - accuracy: 0.6004\n",
      "Epoch 341/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6534 - accuracy: 0.5981\n",
      "Epoch 342/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6516 - accuracy: 0.5988\n",
      "Epoch 343/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6521 - accuracy: 0.6003\n",
      "Epoch 344/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6532 - accuracy: 0.5973\n",
      "Epoch 345/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6517 - accuracy: 0.6023\n",
      "Epoch 346/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6523 - accuracy: 0.5983\n",
      "Epoch 347/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6529 - accuracy: 0.5990\n",
      "Epoch 348/500\n",
      "49193/49193 [==============================] - 36s 736us/step - loss: 0.6525 - accuracy: 0.5997\n",
      "Epoch 349/500\n",
      "49193/49193 [==============================] - 36s 726us/step - loss: 0.6531 - accuracy: 0.5982\n",
      "Epoch 350/500\n",
      "49193/49193 [==============================] - 36s 737us/step - loss: 0.6523 - accuracy: 0.6016\n",
      "Epoch 351/500\n",
      "49193/49193 [==============================] - 36s 733us/step - loss: 0.6521 - accuracy: 0.5991\n",
      "Epoch 352/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6526 - accuracy: 0.5967\n",
      "Epoch 353/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6526 - accuracy: 0.5980\n",
      "Epoch 354/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6532 - accuracy: 0.5982\n",
      "Epoch 355/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6531 - accuracy: 0.5957\n",
      "Epoch 356/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6527 - accuracy: 0.5998\n",
      "Epoch 357/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6512 - accuracy: 0.6000\n",
      "Epoch 358/500\n",
      "49193/49193 [==============================] - 36s 726us/step - loss: 0.6528 - accuracy: 0.5986\n",
      "Epoch 359/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6526 - accuracy: 0.6013\n",
      "Epoch 360/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6533 - accuracy: 0.5981\n",
      "Epoch 361/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6526 - accuracy: 0.5996\n",
      "Epoch 362/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6522 - accuracy: 0.6004\n",
      "Epoch 363/500\n",
      "49193/49193 [==============================] - 36s 735us/step - loss: 0.6519 - accuracy: 0.5997\n",
      "Epoch 364/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6526 - accuracy: 0.5969\n",
      "Epoch 365/500\n",
      "49193/49193 [==============================] - 36s 731us/step - loss: 0.6531 - accuracy: 0.5967\n",
      "Epoch 366/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6520 - accuracy: 0.6012\n",
      "Epoch 367/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6519 - accuracy: 0.5992\n",
      "Epoch 368/500\n",
      "49193/49193 [==============================] - 36s 727us/step - loss: 0.6528 - accuracy: 0.5987\n",
      "Epoch 369/500\n",
      "49193/49193 [==============================] - 36s 726us/step - loss: 0.6512 - accuracy: 0.5989\n",
      "Epoch 370/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6521 - accuracy: 0.5982\n",
      "Epoch 371/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6520 - accuracy: 0.5989\n",
      "Epoch 372/500\n",
      "49193/49193 [==============================] - 36s 729us/step - loss: 0.6523 - accuracy: 0.5989\n",
      "Epoch 373/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6526 - accuracy: 0.6001\n",
      "Epoch 374/500\n",
      "49193/49193 [==============================] - 36s 728us/step - loss: 0.6522 - accuracy: 0.5998\n",
      "Epoch 375/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6531 - accuracy: 0.5970\n",
      "Epoch 376/500\n",
      "49193/49193 [==============================] - 36s 730us/step - loss: 0.6524 - accuracy: 0.5994\n",
      "Epoch 377/500\n",
      "49193/49193 [==============================] - 36s 741us/step - loss: 0.6522 - accuracy: 0.6013\n",
      "Epoch 378/500\n",
      "22600/49193 [============>.................] - ETA: 19s - loss: 0.6513 - accuracy: 0.6014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-99bc7899982a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm.fit(x_train,\n",
    "    labels_train,\n",
    "    epochs = 500,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save('lstm_model_ZV573.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_=lstm.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5422    0.8278    0.6552      6253\n",
      "           1     0.6088    0.2772    0.3810      6046\n",
      "\n",
      "   micro avg     0.5571    0.5571    0.5571     12299\n",
      "   macro avg     0.5755    0.5525    0.5181     12299\n",
      "weighted avg     0.5749    0.5571    0.5204     12299\n",
      "\n",
      "0.5571184649158468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels_test, pred_,digits=4))\n",
    "print(accuracy_score(labels_test, pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
