{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained weak models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/final/ZVe44/trainedXGB0.pkl\n",
      "../models/final/ZVe44/trainedXGB1.pkl\n",
      "../models/final/ZVe44/trainedXGB2.pkl\n",
      "../models/final/ZVe44/trainedXGB3.pkl\n",
      "../models/final/ZVe44/trainedXGB4.pkl\n",
      "../models/final/ZVe44/trainedXGB5.pkl\n",
      "pos\n",
      "not found\n",
      "pos\n",
      "pos\n",
      "not found\n",
      "not found\n",
      "../models/final/ZVe44/trainedXGB12.pkl\n",
      "../models/final/ZVe44/trainedXGB13.pkl\n",
      "../models/final/ZVe44/trainedXGB14.pkl\n",
      "../models/final/ZVe44/trainedXGB15.pkl\n",
      "../models/final/ZVe44/trainedXGB16.pkl\n",
      "../models/final/ZVe44/trainedXGB17.pkl\n",
      "../models/final/ZVe44/trainedXGB18.pkl\n",
      "../models/final/ZVe44/trainedXGB19.pkl\n",
      "../models/final/ZVe44/trainedXGB20.pkl\n",
      "../models/final/ZVe44/trainedXGB21.pkl\n",
      "../models/final/ZVe44/trainedXGB22.pkl\n",
      "../models/final/ZVe44/trainedXGB23.pkl\n",
      "../models/final/ZVe44/trainedXGB24.pkl\n",
      "../models/final/ZVe44/trainedXGB25.pkl\n",
      "../models/final/ZVe44/trainedXGB26.pkl\n",
      "pos\n",
      "../models/final/ZVe44/trainedXGB28.pkl\n",
      "../models/final/ZVe44/trainedXGB29.pkl\n",
      "../models/final/ZVe44/trainedXGB30.pkl\n",
      "../models/final/ZVe44/trainedXGB31.pkl\n",
      "../models/final/ZVe44/trainedXGB32.pkl\n",
      "../models/final/ZVe44/trainedXGB33.pkl\n",
      "../models/final/ZVe44/trainedXGB34.pkl\n",
      "../models/final/ZVe44/trainedXGB35.pkl\n",
      "../models/final/ZV573/trainedXGB0.pkl\n",
      "../models/final/ZV573/trainedXGB1.pkl\n",
      "../models/final/ZV573/trainedXGB2.pkl\n",
      "../models/final/ZV573/trainedXGB3.pkl\n",
      "../models/final/ZV573/trainedXGB4.pkl\n",
      "../models/final/ZV573/trainedXGB5.pkl\n",
      "../models/final/ZV573/trainedXGB6.pkl\n",
      "../models/final/ZV573/trainedXGB7.pkl\n",
      "neg\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "../models/final/ZV573/trainedXGB12.pkl\n",
      "../models/final/ZV573/trainedXGB13.pkl\n",
      "../models/final/ZV573/trainedXGB14.pkl\n",
      "../models/final/ZV573/trainedXGB15.pkl\n",
      "../models/final/ZV573/trainedXGB16.pkl\n",
      "../models/final/ZV573/trainedXGB17.pkl\n",
      "../models/final/ZV573/trainedXGB18.pkl\n",
      "../models/final/ZV573/trainedXGB19.pkl\n",
      "../models/final/ZV573/trainedXGB20.pkl\n",
      "../models/final/ZV573/trainedXGB21.pkl\n",
      "../models/final/ZV573/trainedXGB22.pkl\n",
      "../models/final/ZV573/trainedXGB23.pkl\n",
      "../models/final/ZV573/trainedXGB24.pkl\n",
      "../models/final/ZV573/trainedXGB25.pkl\n",
      "../models/final/ZV573/trainedXGB26.pkl\n",
      "neg\n",
      "../models/final/ZV573/trainedXGB28.pkl\n",
      "../models/final/ZV573/trainedXGB29.pkl\n",
      "../models/final/ZV573/trainedXGB30.pkl\n",
      "../models/final/ZV573/trainedXGB31.pkl\n",
      "../models/final/ZV573/trainedXGB32.pkl\n",
      "../models/final/ZV573/trainedXGB33.pkl\n",
      "../models/final/ZV573/trainedXGB34.pkl\n",
      "../models/final/ZV573/trainedXGB35.pkl\n",
      "../models/final/ZV63d/trainedXGB0.pkl\n",
      "../models/final/ZV63d/trainedXGB1.pkl\n",
      "../models/final/ZV63d/trainedXGB2.pkl\n",
      "../models/final/ZV63d/trainedXGB3.pkl\n",
      "../models/final/ZV63d/trainedXGB4.pkl\n",
      "../models/final/ZV63d/trainedXGB5.pkl\n",
      "../models/final/ZV63d/trainedXGB6.pkl\n",
      "../models/final/ZV63d/trainedXGB7.pkl\n",
      "../models/final/ZV63d/trainedXGB8.pkl\n",
      "neg\n",
      "not found\n",
      "not found\n",
      "../models/final/ZV63d/trainedXGB12.pkl\n",
      "../models/final/ZV63d/trainedXGB13.pkl\n",
      "../models/final/ZV63d/trainedXGB14.pkl\n",
      "../models/final/ZV63d/trainedXGB15.pkl\n",
      "../models/final/ZV63d/trainedXGB16.pkl\n",
      "../models/final/ZV63d/trainedXGB17.pkl\n",
      "../models/final/ZV63d/trainedXGB18.pkl\n",
      "../models/final/ZV63d/trainedXGB19.pkl\n",
      "../models/final/ZV63d/trainedXGB20.pkl\n",
      "../models/final/ZV63d/trainedXGB21.pkl\n",
      "../models/final/ZV63d/trainedXGB22.pkl\n",
      "../models/final/ZV63d/trainedXGB23.pkl\n",
      "../models/final/ZV63d/trainedXGB24.pkl\n",
      "pos\n",
      "not found\n",
      "pos\n",
      "../models/final/ZV63d/trainedXGB28.pkl\n",
      "../models/final/ZV63d/trainedXGB29.pkl\n",
      "../models/final/ZV63d/trainedXGB30.pkl\n",
      "../models/final/ZV63d/trainedXGB31.pkl\n",
      "../models/final/ZV63d/trainedXGB32.pkl\n",
      "../models/final/ZV63d/trainedXGB33.pkl\n",
      "../models/final/ZV63d/trainedXGB34.pkl\n",
      "../models/final/ZV63d/trainedXGB35.pkl\n",
      "../models/final/ZVfd4/trainedXGB0.pkl\n",
      "../models/final/ZVfd4/trainedXGB1.pkl\n",
      "../models/final/ZVfd4/trainedXGB2.pkl\n",
      "../models/final/ZVfd4/trainedXGB3.pkl\n",
      "pos\n",
      "not found\n",
      "../models/final/ZVfd4/trainedXGB6.pkl\n",
      "pos\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "../models/final/ZVfd4/trainedXGB12.pkl\n",
      "../models/final/ZVfd4/trainedXGB13.pkl\n",
      "../models/final/ZVfd4/trainedXGB14.pkl\n",
      "../models/final/ZVfd4/trainedXGB15.pkl\n",
      "../models/final/ZVfd4/trainedXGB16.pkl\n",
      "../models/final/ZVfd4/trainedXGB17.pkl\n",
      "../models/final/ZVfd4/trainedXGB18.pkl\n",
      "../models/final/ZVfd4/trainedXGB19.pkl\n",
      "../models/final/ZVfd4/trainedXGB20.pkl\n",
      "../models/final/ZVfd4/trainedXGB21.pkl\n",
      "../models/final/ZVfd4/trainedXGB22.pkl\n",
      "../models/final/ZVfd4/trainedXGB23.pkl\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "../models/final/ZVfd4/trainedXGB28.pkl\n",
      "../models/final/ZVfd4/trainedXGB29.pkl\n",
      "../models/final/ZVfd4/trainedXGB30.pkl\n",
      "../models/final/ZVfd4/trainedXGB31.pkl\n",
      "../models/final/ZVfd4/trainedXGB32.pkl\n",
      "../models/final/ZVfd4/trainedXGB33.pkl\n",
      "../models/final/ZVfd4/trainedXGB34.pkl\n",
      "../models/final/ZVfd4/trainedXGB35.pkl\n",
      "../models/final/ZVa9c/trainedXGB0.pkl\n",
      "../models/final/ZVa9c/trainedXGB1.pkl\n",
      "../models/final/ZVa9c/trainedXGB2.pkl\n",
      "../models/final/ZVa9c/trainedXGB3.pkl\n",
      "../models/final/ZVa9c/trainedXGB4.pkl\n",
      "../models/final/ZVa9c/trainedXGB5.pkl\n",
      "../models/final/ZVa9c/trainedXGB6.pkl\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "../models/final/ZVa9c/trainedXGB12.pkl\n",
      "../models/final/ZVa9c/trainedXGB13.pkl\n",
      "../models/final/ZVa9c/trainedXGB14.pkl\n",
      "../models/final/ZVa9c/trainedXGB15.pkl\n",
      "../models/final/ZVa9c/trainedXGB16.pkl\n",
      "../models/final/ZVa9c/trainedXGB17.pkl\n",
      "../models/final/ZVa9c/trainedXGB18.pkl\n",
      "../models/final/ZVa9c/trainedXGB19.pkl\n",
      "../models/final/ZVa9c/trainedXGB20.pkl\n",
      "../models/final/ZVa9c/trainedXGB21.pkl\n",
      "../models/final/ZVa9c/trainedXGB22.pkl\n",
      "../models/final/ZVa9c/trainedXGB23.pkl\n",
      "neg\n",
      "not found\n",
      "pos\n",
      "neg\n",
      "../models/final/ZVa9c/trainedXGB28.pkl\n",
      "../models/final/ZVa9c/trainedXGB29.pkl\n",
      "../models/final/ZVa9c/trainedXGB30.pkl\n",
      "../models/final/ZVa9c/trainedXGB31.pkl\n",
      "../models/final/ZVa9c/trainedXGB32.pkl\n",
      "../models/final/ZVa9c/trainedXGB33.pkl\n",
      "../models/final/ZVa9c/trainedXGB34.pkl\n",
      "../models/final/ZVa9c/trainedXGB35.pkl\n",
      "../models/final/ZVa78/trainedXGB0.pkl\n",
      "../models/final/ZVa78/trainedXGB1.pkl\n",
      "../models/final/ZVa78/trainedXGB2.pkl\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB4.pkl\n",
      "../models/final/ZVa78/trainedXGB5.pkl\n",
      "pos\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB12.pkl\n",
      "../models/final/ZVa78/trainedXGB13.pkl\n",
      "../models/final/ZVa78/trainedXGB14.pkl\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB16.pkl\n",
      "../models/final/ZVa78/trainedXGB17.pkl\n",
      "../models/final/ZVa78/trainedXGB18.pkl\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB20.pkl\n",
      "../models/final/ZVa78/trainedXGB21.pkl\n",
      "../models/final/ZVa78/trainedXGB22.pkl\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB24.pkl\n",
      "../models/final/ZVa78/trainedXGB25.pkl\n",
      "not found\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB28.pkl\n",
      "../models/final/ZVa78/trainedXGB29.pkl\n",
      "../models/final/ZVa78/trainedXGB30.pkl\n",
      "not found\n",
      "../models/final/ZVa78/trainedXGB32.pkl\n",
      "../models/final/ZVa78/trainedXGB33.pkl\n",
      "../models/final/ZVa78/trainedXGB34.pkl\n",
      "not found\n",
      "../models/final/ZV252/trainedXGB0.pkl\n",
      "../models/final/ZV252/trainedXGB1.pkl\n",
      "../models/final/ZV252/trainedXGB2.pkl\n",
      "../models/final/ZV252/trainedXGB3.pkl\n",
      "not found\n",
      "not found\n",
      "pos\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "../models/final/ZV252/trainedXGB12.pkl\n",
      "../models/final/ZV252/trainedXGB13.pkl\n",
      "../models/final/ZV252/trainedXGB14.pkl\n",
      "../models/final/ZV252/trainedXGB15.pkl\n",
      "../models/final/ZV252/trainedXGB16.pkl\n",
      "../models/final/ZV252/trainedXGB17.pkl\n",
      "../models/final/ZV252/trainedXGB18.pkl\n",
      "../models/final/ZV252/trainedXGB19.pkl\n",
      "neg\n",
      "../models/final/ZV252/trainedXGB21.pkl\n",
      "../models/final/ZV252/trainedXGB22.pkl\n",
      "../models/final/ZV252/trainedXGB23.pkl\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "not found\n",
      "neg\n",
      "../models/final/ZV252/trainedXGB32.pkl\n",
      "../models/final/ZV252/trainedXGB33.pkl\n",
      "../models/final/ZV252/trainedXGB34.pkl\n",
      "../models/final/ZV252/trainedXGB35.pkl\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.svm.classes\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "#path constants\n",
    "train_path = '../data/final/train/'\n",
    "test_path = '../data/final/test/'\n",
    "\n",
    "#type constants\n",
    "vehicle_types = ['ZVe44', 'ZV573', 'ZV63d', 'ZVfd4', 'ZVa9c', 'ZVa78', 'ZV252']\n",
    "\n",
    "#label dataframe\n",
    "label_df = pd.read_csv('../data/final/label.csv', delimiter = ',', encoding = 'utf-8')\n",
    "\n",
    "#sys.path.append(r'D:/ProgramData/Anaconda3/Lib/site-packages/sklearn/svm/')\n",
    "\n",
    "cluster_n = 36\n",
    "\n",
    "ok = 2\n",
    "pos = 1\n",
    "not_found = 0.5\n",
    "neg = 0\n",
    "\n",
    "#load all the cluster oriented models and map them with the corresponding vehicle type\n",
    "saved_model_path = '../models/final/'\n",
    "GBDT_map = dict()\n",
    "for vehicle_type in vehicle_types:\n",
    "    GBDT_list = list()\n",
    "    df=pd.read_csv(saved_model_path+vehicle_type+'/status.csv', sep=',',header=None)\n",
    "    df = df.iloc[:,0].to_numpy()\n",
    "    for i in range(len(df)):\n",
    "        if df[i] == ok:\n",
    "            fn = saved_model_path +vehicle_type+'/trainedXGB'+str(i)+'.pkl'\n",
    "            print(fn)\n",
    "            with open(fn, 'rb') as file:\n",
    "                pickle_model = pickle.load(file)\n",
    "                GBDT_list.append(pickle_model)\n",
    "        elif df[i] == neg:\n",
    "            print('neg')\n",
    "            GBDT_list.append(neg)\n",
    "        elif df[i] == pos:\n",
    "            print('pos')\n",
    "            GBDT_list.append(pos)\n",
    "        else:\n",
    "            print('not found')\n",
    "            GBDT_list.append(not_found)\n",
    "    GBDT_map[vehicle_type] = GBDT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " 1,\n",
       " 0.5,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 0.5,\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " 1,\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, eta=0.05, gamma=2,\n",
       "               gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.0500000007, max_delta_step=0, max_depth=13,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBDT_map['ZVe44']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to generate score tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(filename, label_df):\n",
    "    idx = label_df.loc[label_df['sample_file_name'] == filename]\n",
    "    return idx.iloc[0]['label']\n",
    "\n",
    "feature_thresholds = dict()\n",
    "feature_thresholds[1] = [3000,5000] #engine rpm\n",
    "feature_thresholds[2] = [4500,7000] #oil pump rpm\n",
    "feature_thresholds[7] = [700, 1650, 2500] #displacement current\n",
    "#3x3x4 = 36 clusters\n",
    "\n",
    "cluster_dict = dict()\n",
    "\n",
    "def clear_dict():\n",
    "    global cluster_dict\n",
    "    cluster_dict = dict()\n",
    "    for i in range(36):\n",
    "        cluster_dict[i] = None\n",
    "        \n",
    "def clustering(df, feature_thresholds, keys, cluster_n, this_num):\n",
    "    if len(keys) == 0:\n",
    "        #print('cluster '+str(this_num)+':'+note)\n",
    "        global cluster_dict\n",
    "        if len(df) == 0:\n",
    "            cluster_dict[this_num] = None\n",
    "        else:\n",
    "            cluster_dict[this_num] = df\n",
    "    else:\n",
    "        keys_ = keys.copy()\n",
    "        key = keys_.pop(0)\n",
    "        thresholds = feature_thresholds[key]\n",
    "        prev = 0\n",
    "        cluster_n = int(cluster_n / (len(thresholds)+1))\n",
    "        i = 0\n",
    "        for val in thresholds:\n",
    "            new_df = df[(df.iloc[:,key] > prev) & (df.iloc[:,key] <= val)]\n",
    "            clustering(new_df, feature_thresholds, keys_, cluster_n, this_num + cluster_n*i)\n",
    "            prev = val\n",
    "            i+=1\n",
    "        \n",
    "        new_df = df[df.iloc[:,key] > prev]\n",
    "        clustering(new_df, feature_thresholds, keys_, cluster_n, this_num + cluster_n*i)\n",
    "        i+=1\n",
    "        \n",
    "\n",
    "def feature_tensor_gen(path, label_df, model_list):\n",
    "#path: train_path or test_path\n",
    "#vehicle_type: one string element under vehicle_types = ['ZVe44', 'ZV573', 'ZV63d', 'ZVfd4', 'ZVa9c', 'ZVa78', 'ZV252']\n",
    "    n_cluster = 36\n",
    "    #these are variables to calculate traversing progress (DO NOT CHANGE)\n",
    "    counts_per_percent = int(len(os.listdir(path)) / 100)\n",
    "    percentage_completion = 0\n",
    "    counter = 0\n",
    "    \n",
    "    #pooling result from 50 weak learners then concatenated with the label\n",
    "    feature_tensor = np.empty((0, cluster_n+1))\n",
    "    \n",
    "    #thresholds to categorize data points\n",
    "    feature_thresholds = dict()\n",
    "    feature_thresholds[1] = [3000,5000] #engine rpm\n",
    "    feature_thresholds[2] = [4500,7000] #oil pump rpm\n",
    "    feature_thresholds[7] = [700, 1650, 2500] #displacement current\n",
    "    \n",
    "    global cluster_dict\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        sample_df = pd.read_csv(path + '/' + file, delimiter = ',', encoding = 'utf-8')\n",
    "        n = len(sample_df)\n",
    "        label = getLabel(file, label_df)\n",
    "        feature_vector = list()\n",
    "        clear_dict()\n",
    "        clustering(sample_df, feature_thresholds, list(feature_thresholds.keys()), cluster_n, 0)\n",
    "        \n",
    "        for i in range(cluster_n):\n",
    "            df = cluster_dict[i]\n",
    "            \n",
    "            if df is None or len(df) == 0:\n",
    "                feature_vector.append(np.nan)\n",
    "                continue\n",
    "                \n",
    "            model = model_list[i]\n",
    "            pooling_score = 0\n",
    "            if model == 0:\n",
    "                pooling_score = 0\n",
    "            elif model == 0.5:\n",
    "                pooling_score = np.nan\n",
    "            elif model == 1:\n",
    "                pooling_score = 1\n",
    "            else:\n",
    "                result = model.predict(df.iloc[:,:-1], validate_features=False)\n",
    "                pooling_score = np.average(result)\n",
    "            feature_vector.append(pooling_score)\n",
    "        \n",
    "        feature_vector.append(label)\n",
    "        feature_vector = np.array(feature_vector) \n",
    "        feature_tensor = np.append(feature_tensor, [feature_vector], axis=0)\n",
    "        # --------------------------------------------------------------------------\n",
    "        # NO NEED TO CHANGE ANYTHING BELOW\n",
    "        \n",
    "        #belows are to show traversing progress (DO NOT CHANGE)\n",
    "        counter += 1\n",
    "        if counter == counts_per_percent:\n",
    "            counter = 0\n",
    "            percentage_completion += 1\n",
    "            print('traversing files under', path, ':', percentage_completion, \"%\", end=\"\\r\", flush=True)\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traversing files under ../data/final/train/ZVe44 : 6 %\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-a255bf9de447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicle_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle_type\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_tensor_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvehicle_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGBDT_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtest_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle_type\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_tensor_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvehicle_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGBDT_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-252c8bd903ee>\u001b[0m in \u001b[0;36mfeature_tensor_gen\u001b[1;34m(path, label_df, model_list)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0msample_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mclear_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-252c8bd903ee>\u001b[0m in \u001b[0;36mgetLabel\u001b[1;34m(filename, label_df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_file_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeature_thresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1912\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1914\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1915\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1782\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1783\u001b[1;33m         \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1784\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_tensor = dict()\n",
    "test_tensor = dict()\n",
    "\n",
    "train_path = '../data/final/train/'\n",
    "test_path = '../data/final/test/'\n",
    "\n",
    "for vehicle_type in vehicle_types:\n",
    "\n",
    "    train_tensor[vehicle_type] = feature_tensor_gen(train_path+vehicle_type, label_df, GBDT_map[vehicle_type])\n",
    "    test_tensor[vehicle_type] = feature_tensor_gen(test_path+vehicle_type, label_df, GBDT_map[vehicle_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traversing files under ../data/final/train/ZV252 : 115 %\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output score tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(feature_tensor.shape)\n",
    "tensor_path = '../data/final/feature_tensors'\n",
    "if not os.path.exists(tensor_path):\n",
    "        os.makedirs(tensor_path)\n",
    "for vehicle_type in vehicle_types:\n",
    "    trainset = train_tensor[vehicle_type]\n",
    "    testset = test_tensor[vehicle_type]\n",
    "    np.savetxt(tensor_path+'/'+vehicle_type+\"_train.csv\", trainset, delimiter=\",\")\n",
    "    np.savetxt(tensor_path+'/'+vehicle_type+\"_test.csv\", testset, delimiter=\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/final/feature_tensors/\n",
      "(13883, 37)\n",
      "(3471, 37)\n",
      "(49193, 37)\n",
      "(12299, 37)\n",
      "(3869, 37)\n",
      "(968, 37)\n",
      "(938, 37)\n",
      "(235, 37)\n",
      "(4178, 37)\n",
      "(1045, 37)\n",
      "(8208, 37)\n",
      "(2052, 37)\n",
      "(345, 37)\n",
      "(87, 37)\n"
     ]
    }
   ],
   "source": [
    "path = '../data/final/feature_tensors/'\n",
    "print(path)\n",
    "train_tensor = dict()\n",
    "test_tensor = dict()\n",
    "for vehicle_type in vehicle_types:\n",
    "    train_tensor[vehicle_type] =pd.read_csv(path+vehicle_type+'_train.csv',sep=',',header=None).to_numpy()\n",
    "    print(train_tensor[vehicle_type].shape)\n",
    "    test_tensor[vehicle_type] = pd.read_csv(path+vehicle_type+'_test.csv', sep=',',header=None).to_numpy()\n",
    "    print(test_tensor[vehicle_type].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test report of GBDT kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance report for vehicle type: ZVe44\n",
      "train acc: 0.987682777497659\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6750    0.6672    0.6711      1731\n",
      "         1.0     0.6727    0.6805    0.6766      1740\n",
      "\n",
      "    accuracy                         0.6739      3471\n",
      "   macro avg     0.6739    0.6739    0.6738      3471\n",
      "weighted avg     0.6739    0.6739    0.6739      3471\n",
      "\n",
      "test acc: 0.6738692019590896\n",
      "AUC: 0.67385206876631\n",
      "Model performance report for vehicle type: ZV573\n",
      "train acc: 0.9791637021527453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6441    0.6450    0.6446      6253\n",
      "         1.0     0.6323    0.6315    0.6319      6046\n",
      "\n",
      "    accuracy                         0.6383     12299\n",
      "   macro avg     0.6382    0.6382    0.6382     12299\n",
      "weighted avg     0.6383    0.6383    0.6383     12299\n",
      "\n",
      "test acc: 0.6383445808602325\n",
      "AUC: 0.6382311548346307\n",
      "Model performance report for vehicle type: ZV63d\n",
      "train acc: 0.9966399586456449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7018    0.7435    0.7220       421\n",
      "         1.0     0.7931    0.7569    0.7746       547\n",
      "\n",
      "    accuracy                         0.7510       968\n",
      "   macro avg     0.7474    0.7502    0.7483       968\n",
      "weighted avg     0.7534    0.7510    0.7517       968\n",
      "\n",
      "test acc: 0.7510330578512396\n",
      "AUC: 0.7501617546800298\n",
      "Model performance report for vehicle type: ZVfd4\n",
      "train acc: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8507    0.8261    0.8382       138\n",
      "         1.0     0.7624    0.7938    0.7778        97\n",
      "\n",
      "    accuracy                         0.8128       235\n",
      "   macro avg     0.8066    0.8100    0.8080       235\n",
      "weighted avg     0.8143    0.8128    0.8133       235\n",
      "\n",
      "test acc: 0.8127659574468085\n",
      "AUC: 0.8099506947557149\n",
      "Model performance report for vehicle type: ZVa9c\n",
      "train acc: 0.9988032551460029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7872    0.8057    0.7964       597\n",
      "         1.0     0.7327    0.7098    0.7211       448\n",
      "\n",
      "    accuracy                         0.7646      1045\n",
      "   macro avg     0.7600    0.7578    0.7587      1045\n",
      "weighted avg     0.7639    0.7646    0.7641      1045\n",
      "\n",
      "test acc: 0.7645933014354067\n",
      "AUC: 0.757758285474994\n",
      "Model performance report for vehicle type: ZVa78\n",
      "train acc: 0.9939083820662769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6720    0.7033    0.6873      1011\n",
      "         1.0     0.6982    0.6667    0.6821      1041\n",
      "\n",
      "    accuracy                         0.6847      2052\n",
      "   macro avg     0.6851    0.6850    0.6847      2052\n",
      "weighted avg     0.6853    0.6847    0.6846      2052\n",
      "\n",
      "test acc: 0.6846978557504874\n",
      "AUC: 0.684965380811078\n",
      "Model performance report for vehicle type: ZV252\n",
      "train acc: 0.9971014492753624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8077    0.8571    0.8317        49\n",
      "         1.0     0.8000    0.7368    0.7671        38\n",
      "\n",
      "    accuracy                         0.8046        87\n",
      "   macro avg     0.8038    0.7970    0.7994        87\n",
      "weighted avg     0.8043    0.8046    0.8035        87\n",
      "\n",
      "test acc: 0.8045977011494253\n",
      "AUC: 0.7969924812030076\n",
      "Overal test acc: 0.6638884754675795\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "def performance_summary(vehicle_type, train, test):\n",
    "    print('Model performance report for vehicle type:', vehicle_type)\n",
    "    params = {'booster': 'gbtree', 'eta': 1, 'max_depth': 16, 'gamma' : 1.5}\n",
    "    bst = xgb.XGBClassifier(**params)\n",
    "    bst.fit(train[:,:36],train[:,36])\n",
    "    \n",
    "    y_hat = bst.predict(train[:,:36])\n",
    "    acc = accuracy_score(train[:,36], y_hat)\n",
    "    print('train acc:',acc)\n",
    "    y_hat = bst.predict(test[:,:36])\n",
    "    acc = accuracy_score(test[:,36], y_hat)\n",
    "    print(classification_report(test[:,36], y_hat,digits=4))\n",
    "    print('test acc:',acc)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test[:,36], y_hat, pos_label=1)\n",
    "    print('AUC:',metrics.auc(fpr, tpr))\n",
    "    with open('../models/final/'+vehicle_type+'_integrated_model.pkl', 'wb') as f:\n",
    "                pickle.dump(bst,f)\n",
    "    return acc * test.shape[0]\n",
    "\n",
    "correct = 0\n",
    "nums = 0\n",
    "for vehicle_type in vehicle_types:\n",
    "    nums += test_tensor[vehicle_type].shape[0]\n",
    "    correct += performance_summary(vehicle_type, train_tensor[vehicle_type], test_tensor[vehicle_type])\n",
    "print('Overal test acc:', correct / nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test report of AVG Pooling Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/final/feature_tensors/\n",
      "summary of test accuracy for vehicle type: ZVe44\n",
      "Train acc: 0.8461427645321616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6979    0.6979    0.6979      1731\n",
      "         1.0     0.6994    0.6994    0.6994      1740\n",
      "\n",
      "    accuracy                         0.6986      3471\n",
      "   macro avg     0.6986    0.6986    0.6986      3471\n",
      "weighted avg     0.6986    0.6986    0.6986      3471\n",
      "\n",
      "Test acc: 0.6986459233650245\n",
      "AUC: 0.6986438972887907\n",
      "summary of test accuracy for vehicle type: ZV573\n",
      "Train acc: 0.8176163275262741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6738    0.6387    0.6558      6253\n",
      "         1.0     0.6454    0.6801    0.6623      6046\n",
      "\n",
      "    accuracy                         0.6591     12299\n",
      "   macro avg     0.6596    0.6594    0.6590     12299\n",
      "weighted avg     0.6598    0.6591    0.6590     12299\n",
      "\n",
      "Test acc: 0.6590779738190097\n",
      "AUC: 0.6594262474819232\n",
      "summary of test accuracy for vehicle type: ZV63d\n",
      "Train acc: 0.9441716205737917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7767    0.5534    0.6463       421\n",
      "         1.0     0.7186    0.8775    0.7901       547\n",
      "\n",
      "    accuracy                         0.7366       968\n",
      "   macro avg     0.7476    0.7155    0.7182       968\n",
      "weighted avg     0.7438    0.7366    0.7276       968\n",
      "\n",
      "Test acc: 0.7365702479338843\n",
      "AUC: 0.715478945837151\n",
      "summary of test accuracy for vehicle type: ZVfd4\n",
      "Train acc: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8759    0.8696    0.8727       138\n",
      "         1.0     0.8163    0.8247    0.8205        97\n",
      "\n",
      "    accuracy                         0.8511       235\n",
      "   macro avg     0.8461    0.8472    0.8466       235\n",
      "weighted avg     0.8513    0.8511    0.8512       235\n",
      "\n",
      "Test acc: 0.851063829787234\n",
      "AUC: 0.8471537427162708\n",
      "summary of test accuracy for vehicle type: ZVa9c\n",
      "Train acc: 0.9892292963140259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7586    0.8526    0.8028       597\n",
      "         1.0     0.7647    0.6384    0.6959       448\n",
      "\n",
      "    accuracy                         0.7608      1045\n",
      "   macro avg     0.7616    0.7455    0.7494      1045\n",
      "weighted avg     0.7612    0.7608    0.7570      1045\n",
      "\n",
      "Test acc: 0.7607655502392344\n",
      "AUC: 0.745494586025365\n",
      "summary of test accuracy for vehicle type: ZVa78\n",
      "Train acc: 0.8745126705653021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7088    0.6044    0.6524      1011\n",
      "         1.0     0.6639    0.7589    0.7082      1041\n",
      "\n",
      "    accuracy                         0.6827      2052\n",
      "   macro avg     0.6863    0.6816    0.6803      2052\n",
      "weighted avg     0.6860    0.6827    0.6807      2052\n",
      "\n",
      "Test acc: 0.6827485380116959\n",
      "AUC: 0.6816189067234485\n",
      "summary of test accuracy for vehicle type: ZV252\n",
      "Train acc: 0.9971014492753624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7593    0.8367    0.7961        49\n",
      "         1.0     0.7576    0.6579    0.7042        38\n",
      "\n",
      "    accuracy                         0.7586        87\n",
      "   macro avg     0.7584    0.7473    0.7502        87\n",
      "weighted avg     0.7585    0.7586    0.7560        87\n",
      "\n",
      "Test acc: 0.7586206896551724\n",
      "AUC: 0.7473147153598282\n",
      "Test acc: 0.6799622959765839\n",
      "average accuracy: 0.6799622959765839\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_report(vehicle_type, train, test):\n",
    "    print('summary of test accuracy for vehicle type:', vehicle_type)\n",
    "    arr = np.copy(train)\n",
    "    where_are_NaNs = np.isnan(arr)\n",
    "    arr[where_are_NaNs] = 0.5\n",
    "    scores = np.mean(arr[:,0:36],axis=1)\n",
    "    scores = [1 if num >= 0.5 else 0 for num in scores]\n",
    "    scores = np.array(scores)\n",
    "    acc = accuracy_score(train[:,36], scores)\n",
    "    print('Train acc:', acc)\n",
    "    \n",
    "    arr = np.copy(test)\n",
    "    where_are_NaNs = np.isnan(arr)\n",
    "    arr[where_are_NaNs] = 0.5\n",
    "    scores = np.mean(arr[:,0:36],axis=1)\n",
    "    scores = [1 if num >= 0.5 else 0 for num in scores]\n",
    "    scores = np.array(scores)\n",
    "    acc = accuracy_score(test[:,36], scores)\n",
    "    print(classification_report(test[:,36], scores, digits=4))\n",
    "    print('Test acc:', acc)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test[:,36], scores, pos_label=1)\n",
    "    print('AUC:',metrics.auc(fpr, tpr))\n",
    "    correct = int(acc * test.shape[0])\n",
    "    #print(correct,'/',test.shape[0])\n",
    "    return correct \n",
    "avg_acc = 0\n",
    "for i in range(1):\n",
    "    path = '../data/final/feature_tensors/'\n",
    "    print(path)\n",
    "    train_tensor = dict()\n",
    "    test_tensor = dict()\n",
    "    for vehicle_type in vehicle_types:\n",
    "        train_tensor[vehicle_type] =pd.read_csv(path+vehicle_type+'_train.csv',sep=',',header=None).to_numpy()\n",
    "        test_tensor[vehicle_type] = pd.read_csv(path+vehicle_type+'_test.csv', sep=',',header=None).to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    nums = 0\n",
    "    for vehicle_type in vehicle_types:\n",
    "        nums += test_tensor[vehicle_type].shape[0]\n",
    "        correct += test_report(vehicle_type, train_tensor[vehicle_type], test_tensor[vehicle_type])\n",
    "    avg_acc += (correct / nums)\n",
    "    print('Test acc:', correct / nums)\n",
    "print('average accuracy:', avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
